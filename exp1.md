## First use case

Before executing this experiment check [`exp1/README.m`](main/exp1/README.md) file
`exp1` folder constains the following files:
- Raw data downloaded from Kaggle will be located in `data/downloaded_data`
- Clean raw data will be located in `data/prepared_data`
- `dvc_plots` folder contains some examples of plots created by DVC 
- `evaluation` folder contains all metrics and JSON files used by DVC in creating those plots
- `src` folder contains all source code needed to run the pipeline
- `params.yaml` file contains all the configurable parameters for processing the data and creating the model
- `dvc.yaml` file is the declaration of the dvc pipeline, while `dvc.lock`file contain metadata used by DVC to track pipeline output files
- `my_env.yaml` file is the declaration of an Anaconda envivoronment needed to activate to execute this use case
- Note that by default DVC creates a local `.gitignore` file to ignore big files that must not be tracked by git, such as model.pkl and data/downloaded_data folder, 

### Demo

Here is shown a demo in wich two experiments are execute, one of them in a temp dir and the other locally. Then one of the is uploaded to git remote and delete from workspace. After that it is downloaded from git remote back to workspace

![DVC demo](assets/images/dvc.gif "DVC demo")

### README FILE

``` markdown
This experiment shows an example on how to use DVC along with Git. Airflow executes each DVC command automatically

To execute the experiment some requirements must be satisfied:

    Software - DVC: In this experiment, DVC is used as a command not as Python package, so it cannot be installed with pip or Conda. It can be installed with snap, from the repository or from the binary package. Visit this link for more information.
    Packages and dependencies to install them automatically, run:
        conda env create -f my_env.yaml
        conda activate exp1
    Airflow first time set up: the first time running Airflow in a device, the following commands must be run:
        airflow db init
        airflow users create --username admin --password admin --role Admin --firstname admin --lastname adminLast --email admin.airflow@gmail.com
    Airflow launch: Then run airflow with the following commands in the conda env created above (called exp1):
        airflow webserver
        airflow scheduler
    DAG running: some requeriments must be satisfaced:
        An SSH key is needed and must be registered in git:
            Create SSH key (for a complete tutorial see this link). To do so, run:
                ssh-keygen -t rsa -b 4096 -C "your_email@domain.com" and press enter
                When prompted, choose your passphrase and press enter
                Copy the key, it is located after the following sentence: The key fingerprint is:
                If everything has executed correctly the following command ls ~/.ssh/id_* must prompt any of id_dsa.pub, id_ecdsa.pub, id_ed25519.pub, or id_rsa.pub.
                To check if your SSH key is active run the following command: ssh-add -l. If SSH fingerprint is prompted along with your email, then it is active. If it is not active run the following command: ssh-add then type the passphrase you chose before.
            Register the newly created SSH key in GitHub:
                Access to your GitHub account -> Settings -> SSH and GPG keys -> New SSH Key"
                Choose a title for your key
                In the "Key" field, paste the key you generated before
        This experiment assumes that the commands git init, dvc init and git remote add <name_repo_ssh> <clone link shh> have been executed in the local repository
        Move the dag creation file (src/dag_exp1.py) file to ~/airflow/dags
        Three Airflow variables must be set:
            Airflow variables can be set via two methods:
                Running the command: airflow variables set <key> <value>
                Airflow UI: being logged in as a user with the admin role, in tab Admin->Variables
            path_to_repo_exp1 variable indicates the absolute path where at least src folder, params.yaml and my_env.yaml files are located. This path must be inside the DVC and Git repositories. DAG execution assumes Git and DVC repos are located in parent folder of this path.
            name_repo_ssh variable indicates the name given to the SSH remote in the command git remote add.
            email_to_notify_exp1 variable sets the receiver email to notify success or failure of the tasks executed in Airflow. It can be set to a dummy value (but this value should have the format of an email address) and the feature will not be used. If you want to use this feature, apart from setting this variable to a valid email, an SMTP server must be configured in airflow.cfg. If you want to use your Gmail account, a key generated by Google is needed.

```
